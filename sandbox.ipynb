{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Ensure the save directory exists\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Load the CSV file\n",
    "data_path = \"data/final_data/BU4707_processed.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ‚úÖ Prepare features and target\n",
    "features = [\"Pe_results\", \"Comp_results\", \"TAC_Reading\"]\n",
    "target = \"Sober_classification\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "# ‚úÖ Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ‚úÖ Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).view(-1, 1, 3)  # Reshape for RNN/LSTM\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ‚úÖ Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# ‚úÖ Create DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Define Model Architectures\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=3, hidden_size=50, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])  # Last time step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=50, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Function to Save Models and Losses\n",
    "def save_model_and_losses(model_name, model, train_losses, test_losses):\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f\"{model_name}.pth\"))  # Save model\n",
    "    losses_df = pd.DataFrame({\"Epoch\": range(1, len(train_losses) + 1), \"Train_Loss\": train_losses, \"Test_Loss\": test_losses})\n",
    "    losses_df.to_csv(os.path.join(SAVE_DIR, f\"{model_name}_losses.csv\"), index=False)\n",
    "    print(f\"‚úÖ {model_name} model & losses saved in '{SAVE_DIR}'!\")\n",
    "\n",
    "# ‚úÖ Train and Evaluate Function\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs=10):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if torch.isnan(loss):  # Stop if NaN detected\n",
    "                print(\"‚ö†Ô∏è NaN detected in loss. Stopping training!\")\n",
    "                return\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        test_loss = sum(criterion(model(inputs), labels).item() for inputs, labels in test_loader) / len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    save_model_and_losses(model.__class__.__name__, model, train_losses, test_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training RNN...\n",
      "Epoch [1/10] - Train Loss: 0.6952\n",
      "Epoch [1/10] - Test Loss: 0.6981\n",
      "Epoch [2/10] - Train Loss: 0.6947\n",
      "Epoch [2/10] - Test Loss: 0.6972\n",
      "Epoch [3/10] - Train Loss: 0.6941\n",
      "Epoch [3/10] - Test Loss: 0.6963\n",
      "Epoch [4/10] - Train Loss: 0.6935\n",
      "Epoch [4/10] - Test Loss: 0.6953\n",
      "Epoch [5/10] - Train Loss: 0.6929\n",
      "Epoch [5/10] - Test Loss: 0.6944\n",
      "Epoch [6/10] - Train Loss: 0.6924\n",
      "Epoch [6/10] - Test Loss: 0.6935\n",
      "Epoch [7/10] - Train Loss: 0.6918\n",
      "Epoch [7/10] - Test Loss: 0.6926\n",
      "Epoch [8/10] - Train Loss: 0.6913\n",
      "Epoch [8/10] - Test Loss: 0.6917\n",
      "Epoch [9/10] - Train Loss: 0.6907\n",
      "Epoch [9/10] - Test Loss: 0.6907\n",
      "Epoch [10/10] - Train Loss: 0.6901\n",
      "Epoch [10/10] - Test Loss: 0.6898\n",
      "‚úÖ RNN model & losses saved in 'saved_models'!\n",
      "\n",
      "üöÄ Training LSTM...\n",
      "Epoch [1/10] - Train Loss: 0.6991\n",
      "Epoch [1/10] - Test Loss: 0.7033\n",
      "Epoch [2/10] - Train Loss: 0.6990\n",
      "Epoch [2/10] - Test Loss: 0.7031\n",
      "Epoch [3/10] - Train Loss: 0.6989\n",
      "Epoch [3/10] - Test Loss: 0.7030\n",
      "Epoch [4/10] - Train Loss: 0.6988\n",
      "Epoch [4/10] - Test Loss: 0.7028\n",
      "Epoch [5/10] - Train Loss: 0.6988\n",
      "Epoch [5/10] - Test Loss: 0.7027\n",
      "Epoch [6/10] - Train Loss: 0.6987\n",
      "Epoch [6/10] - Test Loss: 0.7026\n",
      "Epoch [7/10] - Train Loss: 0.6986\n",
      "Epoch [7/10] - Test Loss: 0.7024\n",
      "Epoch [8/10] - Train Loss: 0.6985\n",
      "Epoch [8/10] - Test Loss: 0.7023\n",
      "Epoch [9/10] - Train Loss: 0.6984\n",
      "Epoch [9/10] - Test Loss: 0.7021\n",
      "Epoch [10/10] - Train Loss: 0.6983\n",
      "Epoch [10/10] - Test Loss: 0.7020\n",
      "‚úÖ LSTM model & losses saved in 'saved_models'!\n",
      "\n",
      "üöÄ Training FNN...\n",
      "Epoch [1/10] - Train Loss: 0.7345\n",
      "Epoch [1/10] - Test Loss: 0.7590\n",
      "Epoch [2/10] - Train Loss: 0.7340\n",
      "Epoch [2/10] - Test Loss: 0.7581\n",
      "Epoch [3/10] - Train Loss: 0.7335\n",
      "Epoch [3/10] - Test Loss: 0.7573\n",
      "Epoch [4/10] - Train Loss: 0.7330\n",
      "Epoch [4/10] - Test Loss: 0.7565\n",
      "Epoch [5/10] - Train Loss: 0.7325\n",
      "Epoch [5/10] - Test Loss: 0.7557\n",
      "Epoch [6/10] - Train Loss: 0.7320\n",
      "Epoch [6/10] - Test Loss: 0.7549\n",
      "Epoch [7/10] - Train Loss: 0.7315\n",
      "Epoch [7/10] - Test Loss: 0.7542\n",
      "Epoch [8/10] - Train Loss: 0.7310\n",
      "Epoch [8/10] - Test Loss: 0.7534\n",
      "Epoch [9/10] - Train Loss: 0.7305\n",
      "Epoch [9/10] - Test Loss: 0.7526\n",
      "Epoch [10/10] - Train Loss: 0.7300\n",
      "Epoch [10/10] - Test Loss: 0.7518\n",
      "‚úÖ FNN model & losses saved in 'saved_models'!\n",
      "\n",
      "‚úÖ All models trained and saved!\n",
      "\n",
      "\n",
      "‚ö†Ô∏è No new data file found for predictions.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Train all models\n",
    "models = {\"RNN\": RNN(), \"LSTM\": LSTM(), \"FNN\": FNN()}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüöÄ Training {name}...\")\n",
    "    train_and_evaluate(model, train_loader, test_loader, epochs=10)\n",
    "\n",
    "print(\"\\n‚úÖ All models trained and saved!\\n\")\n",
    "\n",
    "# ‚úÖ Load and Use Models for Predictions\n",
    "def load_model(model_class, model_name):\n",
    "    model_path = os.path.join(SAVE_DIR, f\"{model_name}.pth\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ö†Ô∏è Model file '{model_path}' not found!\")\n",
    "        return None\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "rnn_model = load_model(RNN, \"RNN\")\n",
    "lstm_model = load_model(LSTM, \"LSTM\")\n",
    "fnn_model = load_model(FNN, \"FNN\")\n",
    "\n",
    "# ‚úÖ Load new data and make predictions\n",
    "new_data_path = \"data/final_data/new_data.csv\"\n",
    "if os.path.exists(new_data_path):\n",
    "    df_new = pd.read_csv(new_data_path)\n",
    "    X_new_scaled = scaler.transform(df_new[features].values)\n",
    "    X_new_tensor = torch.tensor(X_new_scaled, dtype=torch.float32).view(-1, 1, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rnn_preds = (torch.sigmoid(rnn_model(X_new_tensor)) > 0.5).float()\n",
    "        lstm_preds = (torch.sigmoid(lstm_model(X_new_tensor)) > 0.5).float()\n",
    "        fnn_preds = (torch.sigmoid(fnn_model(X_new_tensor)) > 0.5).float()\n",
    "\n",
    "    print(\"\\nüìä Predictions on New Data:\")\n",
    "    print(\"RNN:\", rnn_preds.numpy().flatten())\n",
    "    print(\"LSTM:\", lstm_preds.numpy().flatten())\n",
    "    print(\"FNN:\", fnn_preds.numpy().flatten())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No new data file found for predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "    def load_all_participants(self):\n",
    "        \"\"\"Load all participant CSV files\"\"\"\n",
    "        participant_files = list(self.data_dir.glob(\"final_data/*_processed.csv\"))\n",
    "        return {f.stem: pd.read_csv(f) for f in participant_files}\n",
    "    \n",
    "    def load_merged_data(self):\n",
    "        \"\"\"Load merged accelerometer and TAC data\"\"\"\n",
    "        return {\n",
    "            'accelerometer': pd.read_csv(self.data_dir/'merged_data/ccelerometer_data_processed.csv'),\n",
    "            'tac': pd.read_csv(self.data_dir/'merged_data/tac_data_processed.csv')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SobrietyModelTrainer:\n",
    "    def __init__(self, models):\n",
    "        self.models = models  # Dictionary of model instances\n",
    "        self.results = {}\n",
    "        \n",
    "    def train_all_models(self, X_train, y_train, X_test, y_test, epochs=100):\n",
    "        for name, model in self.models.items():\n",
    "            train_loss, test_loss = self._train_model(model, X_train, y_train, X_test, y_test, epochs)\n",
    "            self.results[name] = {\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'total_loss': [t+te for t,te in zip(train_loss, test_loss)]\n",
    "            }\n",
    "            # Save model and learning curves\n",
    "            torch.save(model.state_dict(), f\"{name}.pth\")\n",
    "            pd.DataFrame(self.results[name]).to_csv(f\"{name}_learning_curves.csv\")\n",
    "    \n",
    "    def _train_model(self, model, X_train, y_train, X_test, y_test, epochs):\n",
    "        # Implementation of training loop\n",
    "        pass\n",
    "    \n",
    "    def generate_plots(self, original_data, predictions):\n",
    "        \"\"\"Generate required visualization plots\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Time-series with TAC change\n",
    "        ax1.plot(original_data['timestamp'], original_data['value'], label='Original')\n",
    "        ax1.plot(original_data['timestamp'], predictions, label='Prediction', linestyle='--')\n",
    "        ax1.set_title(\"Time-series with TAC Change\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: Sobriety metric comparison\n",
    "        ax2.bar(['Sober 1 original', 'Sober 0 original', 'Sober 1 pred', 'Sober 0 pred'],\n",
    "                [original_sober1, original_sober0, pred_sober1, pred_sober0])\n",
    "        ax2.set_title(\"Sobriety Metric Comparison\")\n",
    "        \n",
    "        plt.savefig(\"sobriety_comparison.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_confusion_matrices(self, y_true, y_preds):\n",
    "        \"\"\"Generate confusion matrices for all models\"\"\"\n",
    "        for name, y_pred in y_preds.items():\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            # Plot and save confusion matrix\n",
    "            plt.figure()\n",
    "            sns.heatmap(cm, annot=True)\n",
    "            plt.title(f\"Confusion Matrix - {name}\")\n",
    "            plt.savefig(f\"confusion_matrix_{name}.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RCModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.reservoir = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.readout = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        reservoir_out = torch.tanh(self.reservoir(x))\n",
    "        return self.readout(reservoir_out)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.linear(lstm_out[:, -1, :])\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        return self.linear(rnn_out[:, -1, :])\n",
    "\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten time series if needed\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self, data_loader):\n",
    "        self.data_loader = data_loader\n",
    "        self.complexity_metrics = {}\n",
    "        self.entropy_metrics = {}\n",
    "        \n",
    "    def calculate_complexity(self, signal):\n",
    "        \"\"\"Calculate complexity metric\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calculate_permutation_entropy(self, signal):\n",
    "        \"\"\"Calculate permutation entropy\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def evaluate_all_participants(self, models):\n",
    "        results = []\n",
    "        for pid, data in self.data_loader.load_all_participants().items():\n",
    "            pid_results = {'PID': pid}\n",
    "            for name, model in models.items():\n",
    "                # Make predictions\n",
    "                predictions = model.predict(data)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                pid_results[name] = self._calculate_metrics(data, predictions)\n",
    "                \n",
    "                # Generate plots\n",
    "                self._generate_pid_plots(pid, data, predictions, name)\n",
    "            \n",
    "            results.append(pid_results)\n",
    "        \n",
    "        # Save results as table\n",
    "        pd.DataFrame(results).to_csv(\"participant_results.csv\")\n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(self, data, predictions):\n",
    "        \"\"\"Calculate all required metrics\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RCModel.__init__() got an unexpected keyword argument 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# For regression (or 2 for binary classification)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize all models\u001b[39;00m\n\u001b[1;32m      9\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mRCModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m: LSTMModel(\n\u001b[1;32m     16\u001b[0m         input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[1;32m     17\u001b[0m         hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     18\u001b[0m         num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m     19\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m     20\u001b[0m         output_size\u001b[38;5;241m=\u001b[39moutput_size\n\u001b[1;32m     21\u001b[0m     ),\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m'\u001b[39m: RNNModel(\n\u001b[1;32m     23\u001b[0m         input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[1;32m     24\u001b[0m         hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     25\u001b[0m         num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m     26\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m     27\u001b[0m         output_size\u001b[38;5;241m=\u001b[39moutput_size\n\u001b[1;32m     28\u001b[0m     ),\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomNN\u001b[39m\u001b[38;5;124m'\u001b[39m: CustomNN(\n\u001b[1;32m     30\u001b[0m         input_size\u001b[38;5;241m=\u001b[39minput_size,\n\u001b[1;32m     31\u001b[0m         hidden_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m],  \u001b[38;5;66;03m# Two hidden layers\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         output_size\u001b[38;5;241m=\u001b[39moutput_size,\n\u001b[1;32m     33\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: RCModel.__init__() got an unexpected keyword argument 'output_size'"
     ]
    }
   ],
   "source": [
    "# Define model hyperparameters\n",
    "input_size = 3  # For x, y, z accelerometer data\n",
    "hidden_size = 50  # Size of hidden layers\n",
    "num_layers = 2  # For stacked RNN/LSTM\n",
    "dropout = 0.2  # Regularization\n",
    "output_size = 1  # For regression (or 2 for binary classification)\n",
    "\n",
    "# Initialize all models\n",
    "models = {\n",
    "    'RC': RCModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size\n",
    "    ),\n",
    "    'LSTM': LSTMModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        output_size=output_size\n",
    "    ),\n",
    "    'RNN': RNNModel(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        output_size=output_size\n",
    "    ),\n",
    "    'CustomNN': CustomNN(\n",
    "        input_size=input_size,\n",
    "        hidden_sizes=[64, 32],  # Two hidden layers\n",
    "        output_size=output_size,\n",
    "        dropout=dropout\n",
    "    )\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
