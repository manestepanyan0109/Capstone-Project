{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FNN] Epoch 0/100, Loss: 0.6337, Test Loss: 0.6239\n",
      "[FNN] Epoch 10/100, Loss: 0.6267, Test Loss: 0.6169\n",
      "[FNN] Epoch 20/100, Loss: 0.6194, Test Loss: 0.6092\n",
      "[FNN] Epoch 30/100, Loss: 0.6102, Test Loss: 0.6000\n",
      "[FNN] Epoch 40/100, Loss: 0.5985, Test Loss: 0.5884\n",
      "[FNN] Epoch 50/100, Loss: 0.5832, Test Loss: 0.5737\n",
      "[FNN] Epoch 60/100, Loss: 0.5639, Test Loss: 0.5553\n",
      "[FNN] Epoch 70/100, Loss: 0.5396, Test Loss: 0.5328\n",
      "[FNN] Epoch 80/100, Loss: 0.5093, Test Loss: 0.5048\n",
      "[FNN] Epoch 90/100, Loss: 0.4729, Test Loss: 0.4715\n",
      "[FNN] Training complete. Model & loss data saved.\n",
      "[RNN] Epoch 0/100, Loss: 0.6590, Test Loss: 0.6562\n",
      "[RNN] Epoch 10/100, Loss: 0.6393, Test Loss: 0.6365\n",
      "[RNN] Epoch 20/100, Loss: 0.6206, Test Loss: 0.6181\n",
      "[RNN] Epoch 30/100, Loss: 0.6021, Test Loss: 0.6000\n",
      "[RNN] Epoch 40/100, Loss: 0.5834, Test Loss: 0.5818\n",
      "[RNN] Epoch 50/100, Loss: 0.5645, Test Loss: 0.5634\n",
      "[RNN] Epoch 60/100, Loss: 0.5453, Test Loss: 0.5448\n",
      "[RNN] Epoch 70/100, Loss: 0.5260, Test Loss: 0.5261\n",
      "[RNN] Epoch 80/100, Loss: 0.5065, Test Loss: 0.5076\n",
      "[RNN] Epoch 90/100, Loss: 0.4872, Test Loss: 0.4893\n",
      "[RNN] Training complete. Model & loss data saved.\n",
      "[LSTM] Epoch 0/100, Loss: 0.6768, Test Loss: 0.6761\n",
      "[LSTM] Epoch 10/100, Loss: 0.6676, Test Loss: 0.6667\n",
      "[LSTM] Epoch 20/100, Loss: 0.6583, Test Loss: 0.6574\n",
      "[LSTM] Epoch 30/100, Loss: 0.6488, Test Loss: 0.6479\n",
      "[LSTM] Epoch 40/100, Loss: 0.6392, Test Loss: 0.6382\n",
      "[LSTM] Epoch 50/100, Loss: 0.6291, Test Loss: 0.6282\n",
      "[LSTM] Epoch 60/100, Loss: 0.6185, Test Loss: 0.6176\n",
      "[LSTM] Epoch 70/100, Loss: 0.6073, Test Loss: 0.6066\n",
      "[LSTM] Epoch 80/100, Loss: 0.5955, Test Loss: 0.5950\n",
      "[LSTM] Epoch 90/100, Loss: 0.5831, Test Loss: 0.5830\n",
      "[LSTM] Training complete. Model & loss data saved.\n",
      "[CNN] Epoch 0/100, Loss: 0.7122, Test Loss: 0.7032\n",
      "[CNN] Epoch 10/100, Loss: 0.6940, Test Loss: 0.6866\n",
      "[CNN] Epoch 20/100, Loss: 0.6764, Test Loss: 0.6699\n",
      "[CNN] Epoch 30/100, Loss: 0.6594, Test Loss: 0.6535\n",
      "[CNN] Epoch 40/100, Loss: 0.6423, Test Loss: 0.6372\n",
      "[CNN] Epoch 50/100, Loss: 0.6248, Test Loss: 0.6206\n",
      "[CNN] Epoch 60/100, Loss: 0.6066, Test Loss: 0.6035\n",
      "[CNN] Epoch 70/100, Loss: 0.5871, Test Loss: 0.5858\n",
      "[CNN] Epoch 80/100, Loss: 0.5662, Test Loss: 0.5669\n",
      "[CNN] Epoch 90/100, Loss: 0.5440, Test Loss: 0.5469\n",
      "[CNN] Training complete. Model & loss data saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"merged_data.csv\")\n",
    "data = data.dropna()  # Remove all rows with NaN values\n",
    "\n",
    "\n",
    "# Select features and target\n",
    "X = data[['Pe_results', 'Comp_results', 'TAC_Reading']].values\n",
    "y = data['Sober_classification'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Reshape for binary classification\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, y_train, X_test, y_test = X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
    "\n",
    "\n",
    "# ===================== Fully Connected Neural Network (FNN) ===================== #\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No sigmoid here (handled by BCEWithLogitsLoss)\n",
    "        return x  # Keep raw logits\n",
    "\n",
    "fnn_model = FNN().to(device)  # Initialize the FNN model\n",
    "\n",
    "\n",
    "# ===================== Recurrent Neural Network (RNN) ===================== #\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=16, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension -> (batch_size, seq_length=1, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output\n",
    "        return out  # No Sigmoid\n",
    "\n",
    "rnn_model = RNN().to(device)\n",
    "\n",
    "\n",
    "# ===================== Long Short-Term Memory (LSTM) ===================== #\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=16, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch_size, seq_length=1, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out  # No Sigmoid\n",
    "\n",
    "lstm_model = LSTM().to(device)\n",
    "\n",
    "\n",
    "# ===================== Convolutional Neural Network (CNN) ===================== #\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(16, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension -> (batch_size, 1, features)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No Sigmoid\n",
    "        return x  # No Sigmoid\n",
    "\n",
    "cnn_model = CNN().to(device)\n",
    "\n",
    "\n",
    "# ===================== Training Function ===================== #\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, model_name, epochs=100, lr=0.001):\n",
    "    criterion = nn.BCEWithLogitsLoss()  # No sigmoid in model!\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Store losses for visualization\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevents exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'[{model_name}] Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "    # Save losses to CSV\n",
    "    loss_data = pd.DataFrame({\n",
    "        'Epoch': list(range(epochs)),\n",
    "        'Train Loss': train_losses,\n",
    "        'Test Loss': test_losses,\n",
    "        'Total Loss': np.array(train_losses) + np.array(test_losses)  # Sum of both\n",
    "    })\n",
    "\n",
    "    os.makedirs(\"logs\", exist_ok=True)  # Create folder if it doesn't exist\n",
    "    loss_data.to_csv(f'logs/{model_name}_losses.csv', index=False)\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(\"models\", exist_ok=True)  # Create folder for models\n",
    "    torch.save(model.state_dict(), f'models/{model_name}.pth')\n",
    "\n",
    "    print(f'[{model_name}] Training complete. Model & loss data saved.')\n",
    "\n",
    "\n",
    "# ===================== Training Models ===================== #\n",
    "train_model(fnn_model, X_train, y_train, X_test, y_test, model_name=\"FNN\")  \n",
    "train_model(rnn_model, X_train, y_train, X_test, y_test, model_name=\"RNN\")  \n",
    "train_model(lstm_model, X_train, y_train, X_test, y_test, model_name=\"LSTM\")  \n",
    "train_model(cnn_model, X_train, y_train, X_test, y_test, model_name=\"CNN\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Axis</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Pe_results</th>\n",
       "      <th>Comp_results</th>\n",
       "      <th>TAC_Reading</th>\n",
       "      <th>Sober_classification</th>\n",
       "      <th>pid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>1</td>\n",
       "      <td>JR8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966999</td>\n",
       "      <td>0.030278</td>\n",
       "      <td>0.176895</td>\n",
       "      <td>1</td>\n",
       "      <td>JR8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>2</td>\n",
       "      <td>0.961121</td>\n",
       "      <td>0.035584</td>\n",
       "      <td>0.206761</td>\n",
       "      <td>1</td>\n",
       "      <td>JR8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932285</td>\n",
       "      <td>0.065238</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>1</td>\n",
       "      <td>JR8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979987</td>\n",
       "      <td>0.019418</td>\n",
       "      <td>0.176895</td>\n",
       "      <td>1</td>\n",
       "      <td>JR8022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>z</td>\n",
       "      <td>12</td>\n",
       "      <td>0.961497</td>\n",
       "      <td>0.037951</td>\n",
       "      <td>0.134315</td>\n",
       "      <td>1</td>\n",
       "      <td>BK7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>z</td>\n",
       "      <td>13</td>\n",
       "      <td>0.987113</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.148265</td>\n",
       "      <td>1</td>\n",
       "      <td>BK7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>z</td>\n",
       "      <td>14</td>\n",
       "      <td>0.956433</td>\n",
       "      <td>0.039753</td>\n",
       "      <td>0.160774</td>\n",
       "      <td>1</td>\n",
       "      <td>BK7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>z</td>\n",
       "      <td>15</td>\n",
       "      <td>0.915632</td>\n",
       "      <td>0.082337</td>\n",
       "      <td>0.166076</td>\n",
       "      <td>1</td>\n",
       "      <td>BK7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>z</td>\n",
       "      <td>16</td>\n",
       "      <td>0.971516</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>0.171758</td>\n",
       "      <td>1</td>\n",
       "      <td>BK7610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Axis  Unnamed: 1  Pe_results  Comp_results  TAC_Reading  \\\n",
       "0      x           0    0.977528      0.021814     0.088046   \n",
       "1      x           1    0.966999      0.030278     0.176895   \n",
       "2      x           2    0.961121      0.035584     0.206761   \n",
       "3      y           0    0.932285      0.065238     0.088046   \n",
       "4      y           1    0.979987      0.019418     0.176895   \n",
       "..   ...         ...         ...           ...          ...   \n",
       "481    z          12    0.961497      0.037951     0.134315   \n",
       "482    z          13    0.987113      0.012408     0.148265   \n",
       "483    z          14    0.956433      0.039753     0.160774   \n",
       "484    z          15    0.915632      0.082337     0.166076   \n",
       "485    z          16    0.971516      0.028832     0.171758   \n",
       "\n",
       "     Sober_classification     pid  \n",
       "0                       1  JR8022  \n",
       "1                       1  JR8022  \n",
       "2                       1  JR8022  \n",
       "3                       1  JR8022  \n",
       "4                       1  JR8022  \n",
       "..                    ...     ...  \n",
       "481                     1  BK7610  \n",
       "482                     1  BK7610  \n",
       "483                     1  BK7610  \n",
       "484                     1  BK7610  \n",
       "485                     1  BK7610  \n",
       "\n",
       "[481 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "       Accuracy  Precision    Recall        F1  Original_Complexity  \\\n",
      "Model                                                                 \n",
      "FNN    0.864865        1.0  0.522059  0.685990             0.037545   \n",
      "RNN    0.792100        1.0  0.264706  0.418605             0.037545   \n",
      "LSTM   0.785863        1.0  0.242647  0.390533             0.037545   \n",
      "CNN    0.777547        1.0  0.213235  0.351515             0.037545   \n",
      "\n",
      "       Original_PE  Predicted_Complexity  Predicted_PE  \n",
      "Model                                                   \n",
      "FNN       0.124173              0.028059      0.153862  \n",
      "RNN       0.124173              0.019804      0.176093  \n",
      "LSTM      0.124173              0.018726      0.178629  \n",
      "CNN       0.124173              0.020578      0.178850  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# ===================== Updated Prediction and Evaluation Functions ===================== #\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        y_np = y.cpu().numpy()\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        \n",
    "        acc = accuracy_score(y_np, preds_np)\n",
    "        prec = precision_score(y_np, preds_np)\n",
    "        rec = recall_score(y_np, preds_np)\n",
    "        f1 = f1_score(y_np, preds_np)\n",
    "        \n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1}\n",
    "\n",
    "def plot_time_series_comparison(original_data, predictions, model_name):\n",
    "    \"\"\"Plot original vs predicted time series data\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create time index if not exists\n",
    "    if 'time_index' not in original_data.columns:\n",
    "        original_data['time_index'] = np.arange(len(original_data))\n",
    "        predictions['time_index'] = np.arange(len(predictions))\n",
    "    \n",
    "    # Plot 1: TAC Readings comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.lineplot(data=original_data, x='time_index', y='TAC_Reading', \n",
    "                 label='Original TAC', color='blue')\n",
    "    sns.lineplot(data=predictions, x='time_index', y='TAC_Reading', \n",
    "                 label='Predicted TAC', color='red', alpha=0.7)\n",
    "    plt.title('TAC Readings Comparison')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('TAC Value')\n",
    "    \n",
    "    # Plot 2: Sober Classification comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.lineplot(data=original_data, x='time_index', y='Sober_classification', \n",
    "                 label='Original Sober', color='green')\n",
    "    sns.lineplot(data=predictions, x='time_index', y='Predicted_Sober', \n",
    "                 label='Predicted Sober', color='orange', alpha=0.7)\n",
    "    plt.title('Sober Classification Comparison')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Sober (1) / Not Sober (0)')\n",
    "    \n",
    "    # Plot 3: Feature comparison - PE Results\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.lineplot(data=original_data, x='time_index', y='Pe_results', \n",
    "                 label='Original PE', color='purple')\n",
    "    plt.title('Permutation Entropy Over Time')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('PE Value')\n",
    "    \n",
    "    # Plot 4: Feature comparison - Complexity Results\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.lineplot(data=original_data, x='time_index', y='Comp_results', \n",
    "                 label='Original Complexity', color='brown')\n",
    "    plt.title('Complexity Over Time')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Complexity Value')\n",
    "    \n",
    "    plt.suptitle(f'Model: {model_name} - Time Series Analysis')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plt.savefig(f'plots/{model_name}_time_series_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_metrics(original_series, predicted_series):\n",
    "    \"\"\"Calculate complexity and permutation entropy metrics\"\"\"\n",
    "    # Placeholder - implement your actual calculations here\n",
    "    # For demonstration, we'll use simple statistical measures\n",
    "    \n",
    "    orig_complexity = np.std(original_series)  # Replace with actual complexity measure\n",
    "    orig_pe = np.mean(original_series)        # Replace with actual PE calculation\n",
    "    \n",
    "    pred_complexity = np.std(predicted_series)\n",
    "    pred_pe = np.mean(predicted_series)\n",
    "    \n",
    "    return {\n",
    "        'Original_Complexity': orig_complexity,\n",
    "        'Original_PE': orig_pe,\n",
    "        'Predicted_Complexity': pred_complexity,\n",
    "        'Predicted_PE': pred_pe\n",
    "    }\n",
    "\n",
    "# ===================== Main Prediction Pipeline ===================== #\n",
    "\n",
    "def run_time_series_prediction(models_dict, data):\n",
    "    \"\"\"Run predictions for all models on the time series data\"\"\"\n",
    "    # Prepare data\n",
    "    X_all = data[['Pe_results', 'Comp_results', 'TAC_Reading']].values\n",
    "    X_all = scaler.transform(X_all)\n",
    "    X_all_tensor = torch.tensor(X_all, dtype=torch.float32).to(device)\n",
    "    y_all_tensor = torch.tensor(data['Sober_classification'].values, \n",
    "                               dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    results = {}\n",
    "    metrics_comparison = []\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        # Load model if not already loaded\n",
    "        if isinstance(model, str):\n",
    "            model_path = f\"models/{model}.pth\"\n",
    "            model_class = globals()[model]()  # Get model class by name\n",
    "            model_class.load_state_dict(torch.load(model_path))\n",
    "            model_class.to(device)\n",
    "            model = model_class\n",
    "        \n",
    "        # Evaluate on full dataset\n",
    "        model_metrics = evaluate_model(model, X_all_tensor, y_all_tensor)\n",
    "        results[model_name] = model_metrics\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_all_tensor)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            preds = preds.cpu().numpy().flatten()\n",
    "        \n",
    "        # Create prediction dataframe\n",
    "        pred_data = data.copy()\n",
    "        pred_data['Predicted_Sober'] = preds\n",
    "        \n",
    "        # Calculate complexity and PE metrics\n",
    "        tac_metrics = calculate_metrics(\n",
    "            pred_data['TAC_Reading'][pred_data['Sober_classification'] == 1],\n",
    "            pred_data['TAC_Reading'][pred_data['Predicted_Sober'] == 1]\n",
    "        )\n",
    "        \n",
    "        # Store metrics for comparison\n",
    "        metrics_comparison.append({\n",
    "            'Model': model_name,\n",
    "            **model_metrics,\n",
    "            **tac_metrics\n",
    "        })\n",
    "        \n",
    "        # Plot results\n",
    "        plot_time_series_comparison(data, pred_data, model_name)\n",
    "    \n",
    "    # Save metrics and results\n",
    "    pd.DataFrame(results).T.to_csv('logs/model_metrics.csv')\n",
    "    pd.DataFrame(metrics_comparison).to_csv('logs/metrics_comparison.csv', index=False)\n",
    "    \n",
    "    # Print model comparison\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(pd.DataFrame(metrics_comparison).set_index('Model'))\n",
    "    \n",
    "    return pd.DataFrame(metrics_comparison)\n",
    "\n",
    "# ===================== Run the Pipeline ===================== #\n",
    "\n",
    "# Dictionary of models to evaluate\n",
    "models_dict = {\n",
    "    'FNN': fnn_model,\n",
    "    'RNN': rnn_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'CNN': cnn_model\n",
    "}\n",
    "\n",
    "# Run the prediction pipeline\n",
    "results = run_time_series_prediction(models_dict, data)\n",
    "\n",
    "# ===================== Additional Visualization ===================== #\n",
    "\n",
    "def plot_metrics_comparison(metrics_df):\n",
    "    \"\"\"Plot comparison of model metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    plt.subplot(2, 2, 1)\n",
    "    metrics_df[metrics_to_plot].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Classification Metrics Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot complexity metrics\n",
    "    plt.subplot(2, 2, 2)\n",
    "    metrics_df[['Original_Complexity', 'Predicted_Complexity']].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Complexity Comparison')\n",
    "    plt.ylabel('Complexity Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot PE metrics\n",
    "    plt.subplot(2, 2, 3)\n",
    "    metrics_df[['Original_PE', 'Predicted_PE']].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Permutation Entropy Comparison')\n",
    "    plt.ylabel('PE Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/metrics_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the metrics comparison\n",
    "plot_metrics_comparison(results.set_index('Model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mane/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/mane/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/mane/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/mane/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance by PID (F1 Scores):\n",
      "PID      BK7610  BU4707    CC6740  DC6359  DK3500  HV0618  JB3156  JR8022  \\\n",
      "Model                                                                       \n",
      "CNN    0.421053     0.0  0.500000     0.0     0.0     0.0     0.0     0.8   \n",
      "FNN    0.666667     0.0  0.666667     1.0     0.0     0.8     0.0     0.8   \n",
      "LSTM   0.461538     0.0  0.400000     0.0     0.0     0.0     0.0     0.8   \n",
      "RNN    0.500000     0.0  0.588235     0.0     0.0     0.0     0.0     0.8   \n",
      "\n",
      "PID      MC7070    MJ8002  PC6771    SA0297    SF3079   Average  \n",
      "Model                                                            \n",
      "CNN    0.125000  0.400000     0.0  0.363636  0.312500  0.224784  \n",
      "FNN    0.888889  0.666667     0.0  0.875000  0.682927  0.542063  \n",
      "LSTM   0.333333  0.400000     0.0  0.500000  0.363636  0.250654  \n",
      "RNN    0.333333  0.400000     0.0  0.500000  0.363636  0.268093  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ===================== Data Preparation ===================== #\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"merged_data.csv\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Rename columns to handle case sensitivity\n",
    "data = data.rename(columns={'pid': 'PID'})  # Ensure consistent column naming\n",
    "\n",
    "# Verify PID column exists\n",
    "if 'PID' not in data.columns:\n",
    "    raise ValueError(\"PID column not found in data. Available columns: \" + \", \".join(data.columns))\n",
    "\n",
    "# Create time index for each PID group\n",
    "data['time_index'] = data.groupby('PID').cumcount()\n",
    "\n",
    "# ===================== Evaluation Functions ===================== #\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        y_np = y.cpu().numpy()\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        \n",
    "        acc = accuracy_score(y_np, preds_np)\n",
    "        prec = precision_score(y_np, preds_np)\n",
    "        rec = recall_score(y_np, preds_np)\n",
    "        f1 = f1_score(y_np, preds_np)\n",
    "        \n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1}\n",
    "\n",
    "# ===================== Visualization Functions ===================== #\n",
    "\n",
    "def plot_pid_comparison(pid_data, model_name):\n",
    "    \"\"\"Plot comparison for a single PID\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: TAC Readings\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(pid_data['time_index'], pid_data['TAC_Reading'], \n",
    "             label='Original', color='blue')\n",
    "    plt.plot(pid_data['time_index'][pid_data['Predicted_Sober'] == 1], \n",
    "             pid_data['TAC_Reading'][pid_data['Predicted_Sober'] == 1],\n",
    "             'o', label='Predicted Sober', color='green', alpha=0.7)\n",
    "    plt.plot(pid_data['time_index'][pid_data['Predicted_Sober'] == 0], \n",
    "             pid_data['TAC_Reading'][pid_data['Predicted_Sober'] == 0],\n",
    "             'x', label='Predicted Not Sober', color='red', alpha=0.7)\n",
    "    plt.title(f'TAC Readings for PID {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('TAC Value')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Sober Classification\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.step(pid_data['time_index'], pid_data['Sober_classification'], \n",
    "             where='post', label='Original', color='blue')\n",
    "    plt.step(pid_data['time_index'], pid_data['Predicted_Sober'], \n",
    "             where='post', label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Sober Classification for PID {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Sober (1) / Not Sober (0)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: PE Results\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(pid_data['time_index'], pid_data['Pe_results'], \n",
    "             label='Original', color='purple')\n",
    "    plt.title(f'Permutation Entropy for PID {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('PE Value')\n",
    "    \n",
    "    # Plot 4: Complexity Results\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(pid_data['time_index'], pid_data['Comp_results'], \n",
    "             label='Original', color='brown')\n",
    "    plt.title(f'Complexity for PID {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Complexity Value')\n",
    "    \n",
    "    # Plot 5: Sober vs TAC\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.scatter(pid_data['TAC_Reading'], pid_data['Sober_classification'],\n",
    "               label='Original', color='blue', alpha=0.5)\n",
    "    plt.scatter(pid_data['TAC_Reading'], pid_data['Predicted_Sober'],\n",
    "               label='Predicted', color='red', alpha=0.5)\n",
    "    plt.title(f'TAC vs Sober Classification for PID {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.xlabel('TAC Reading')\n",
    "    plt.ylabel('Sober Classification')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.suptitle(f'Model: {model_name} - PID: {pid_data[\"PID\"].iloc[0]}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    plt.savefig(f'plots/{model_name}_PID_{pid_data[\"PID\"].iloc[0]}.png')\n",
    "    plt.close()\n",
    "\n",
    "# ===================== Main Prediction Pipeline ===================== #\n",
    "\n",
    "def run_pid_analysis(models_dict, data):\n",
    "    \"\"\"Run analysis for each PID and model\"\"\"\n",
    "    # Prepare results storage\n",
    "    pid_results = []\n",
    "    model_metrics = []\n",
    "    \n",
    "    # Process each model\n",
    "    for model_name, model in models_dict.items():\n",
    "        # Load model if needed\n",
    "        if isinstance(model, str):\n",
    "            model_path = f\"models/{model}.pth\"\n",
    "            model_class = globals()[model]()\n",
    "            model_class.load_state_dict(torch.load(model_path))\n",
    "            model_class.to(device)\n",
    "            model = model_class\n",
    "        \n",
    "        # Prepare data\n",
    "        X = data[['Pe_results', 'Comp_results', 'TAC_Reading']].values\n",
    "        X = scaler.transform(X)\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_tensor)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            data['Predicted_Sober'] = preds.cpu().numpy().flatten()\n",
    "        \n",
    "        # Evaluate overall performance\n",
    "        y_tensor = torch.tensor(data['Sober_classification'].values, \n",
    "                               dtype=torch.float32).view(-1, 1).to(device)\n",
    "        overall_metrics = evaluate_model(model, X_tensor, y_tensor)\n",
    "        model_metrics.append({'Model': model_name, **overall_metrics})\n",
    "        \n",
    "        # Analyze by PID\n",
    "        for pid in data['PID'].unique():\n",
    "            pid_data = data[data['PID'] == pid].copy()\n",
    "            \n",
    "            # Calculate metrics for this PID\n",
    "            pid_metrics = {\n",
    "                'Model': model_name,\n",
    "                'PID': pid,\n",
    "                'Accuracy': accuracy_score(pid_data['Sober_classification'], pid_data['Predicted_Sober']),\n",
    "                'F1': f1_score(pid_data['Sober_classification'], pid_data['Predicted_Sober'])\n",
    "            }\n",
    "            pid_results.append(pid_metrics)\n",
    "            \n",
    "            # Create plots for this PID\n",
    "            plot_pid_comparison(pid_data, model_name)\n",
    "    \n",
    "    # Save results\n",
    "    pd.DataFrame(model_metrics).to_csv('logs/model_metrics.csv', index=False)\n",
    "    pid_results_df = pd.DataFrame(pid_results)\n",
    "    pid_results_df.to_csv('logs/pid_results.csv', index=False)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_table = pid_results_df.pivot(index='Model', columns='PID', values='F1')\n",
    "    summary_table['Average'] = pid_results_df.groupby('Model')['F1'].mean()\n",
    "    \n",
    "    print(\"\\nModel Performance by PID (F1 Scores):\")\n",
    "    print(summary_table)\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "# ===================== Run the Analysis ===================== #\n",
    "\n",
    "# Dictionary of models to evaluate\n",
    "models_dict = {\n",
    "    'FNN': fnn_model,\n",
    "    'RNN': rnn_model,\n",
    "    'LSTM': lstm_model,\n",
    "    'CNN': cnn_model\n",
    "}\n",
    "\n",
    "# Run the PID analysis\n",
    "results_table = run_pid_analysis(models_dict, data)\n",
    "\n",
    "# ===================== Additional Visualizations ===================== #\n",
    "\n",
    "def plot_model_comparison(results_df):\n",
    "    \"\"\"Plot model comparison across PIDs\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Melt the dataframe for seaborn\n",
    "    melted_df = results_df.reset_index().melt(id_vars='Model', \n",
    "                                            value_vars=[col for col in results_df.columns if col != 'Average'],\n",
    "                                            var_name='PID', value_name='F1 Score')\n",
    "    \n",
    "    # Plot F1 scores by model and PID\n",
    "    sns.barplot(data=melted_df, x='PID', y='F1 Score', hue='Model')\n",
    "    plt.title('Model Performance by PID')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig('plots/model_comparison_by_pid.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the comparison\n",
    "plot_model_comparison(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
